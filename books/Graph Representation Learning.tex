
\chapter{图表示学习}
\Large\textbf{Graph Representation Learning\footnote{本笔记亦主要参考Jure Leskovec图机器学习讲义(Stanford CS224W，2021)}}
\par \emph{William L. Hamilton} \normalsize

\section{导论}

\par 图是描述和分析有关联(交互)实体的通用语言。图机器学习的核心问题：如何利用关系结构进行更好的预测？

\par 任务分类：
\begin{itemize}
    \item 节点级(Node level)：节点分类 （当已知图中部分节点类别，求其它节点类别时，称半监督节点分类）。
    \item 链接级(Link level)：链接预测（根据已知边预测其它节点对是否连边，如知识图谱补全，推荐系统中预测用户与物品的连接）
    \item 子图级(Subgraph level)：社区探测
    \item 图级(Graph level)：图分类（如分子性质预测），图生成，图演化模拟
\end{itemize}

\par 二部图的投影图(folded/projected graph)：设两个顶点独立集$U,V$，对$U$的投影图节点为$U$，$u_1,u_2\in U$连边当且仅当存在$v\in V$，$u_1,u_2$在二部图中都与$v$相连。

\par 异质图(heterogeneous graph)：区分不同类型节点和边。 $G=(V,E,R,T)$，边定义为$(v_i,r_{ij},v_j)\in E$, $r_{ij}\in R$是边类型，$T(v_i)$是节点类型。在知识图谱中常用。

\par 稀疏图(sparse graph)：平均度远小于$N-1$。大多数现实世界的网络都是稀疏图。

\section{特征提取}

\par 传统图机器学习方法需要人为定义节点、节点对、图的特征（表示为向量）用于训练。

\subsection{节点特征}

度、中心性、Pagerank反映节点的重要性。度、聚集系数、图元度向量描述节点所在局部的拓扑属性。

\par \textbf{度}(degree).

\par \textbf{中心性}(centrality): 衡量节点在图中的重要性。
\par 特征向量中心性(eigenvector centrality)满足
\begin{equation}
    c_v=\frac{1}{\lambda}\sum_{u\in N(v)} c_u.
\end{equation}
其中$A$为简单无向图的邻接矩阵。上式化为$\lambda \mathbf{c}=A\mathbf{c}$，即$\mathbf{c}$为$A$的特征向量。一般取最大特征值对应特征向量\footnote{无向连通图的邻接矩阵不可约。Perron-Frobenius定理指出，不可约非负矩阵最大特征值为正且代数重数为1，从而该特征向量在乘一个常数意义下唯一。}。
\par 介中心性(betweenness centrality)：对节点$v$，定义为一对节点所有最短路径中包含$v$的路径所占比例，对所有不含$v$的节点对求和。
\par 接近中心性(closeness centrality)：对给定节点，定义为该节点到其余节点最短路径长度之和的倒数。

\par \textbf{聚集系数}(clustering coefficient)：对节点$v$，考虑其邻接节点（设有$k_v$个）的导出子图，其边数与$\binom{k_v}{2}$之比。反映邻域连通程度。

\par \textbf{图元度向量}(graphlet degree vector)：图元即较小的连通有根图。令根节点与给定节点对应，对每个图元统计与之同构的导出子图的数量\footnote{如考虑节点数2-5的图元，得到73维向量。}。事实上度统计$K_2$的数量，聚集系数统计$K_3$的数量。

\par \textbf{Pagerank}：又称Google算法，用链接分析(link analysis)度量有向图节点重要性的代表算法。设节点$i$的重要性为$r_i$，出度为$d_i$，并令
\begin{equation}
    r_j=\sum_{i\to j}\frac{r_i}{d_i}.
\end{equation}
定义随机邻接矩阵$M$，$M_{ji}=I[i\to j] \frac{1}{d_i}$，则其列和为1；记重要性向量为$\mathbf{r}$，则有
\begin{equation}
    \mathbf{r}=M\mathbf{r}.
\end{equation}
考虑无偏随机游走，$M$恰好为其转移概率矩阵，归一化后的$\mathbf{r}$为其平稳分布；$\mathbf{r}$同时为$M$特征值为1的特征向量。
\par 利用乘方迭代法求解$\mathbf{r}$：可以从均匀分布出发通过乘转移矩阵至收敛得到。特别地，到达出度为0的节点时，下一步以$1/N$概率到达每个节点；为处理闭集（不指向集合外的顶点集合），以$\beta$概率沿基本规则运动，以$1-\beta$概率随机到访任一节点。$\beta$通常取0.8-0.9。

\par 个性化Pagerank (Personalized Pagerank)度量各节点到特定节点集合$S$的邻近性，随机游走时每步以$\alpha$概率回到$S$中随机节点（可指定不同概率）。当$S$中只含一个节点时，称为带重启的随机游走(random walk with restarts)。可用于在用户-商品二部图中确定某一（些）商品的相似商品（此时每步游走事实上走两步，从商品到商品）。

\subsection{节点对特征}

\par \textbf{距离}：网络最短距离。
\par \textbf{局部邻域重叠}：公共邻居数；
\par Jaccard指数：
\begin{equation}
    \frac{\vert N(v_1)\cap N(v_2)\vert}{\vert N(v_1)\cup N(v_2)\vert}
\end{equation}
Adamic-Adar指数：
\begin{equation}
    \sum_{u\in N(v_1)\cap N(v_2)} \frac{1}{\log k_u}.
\end{equation}
局限性是如果没有公共邻居，局部邻域重叠总是0.

\par \textbf{Katz指数}：对简单无向图邻接矩阵乘方，可得到一对节点间给定长度的路径数。设$\beta \in (0,1)$为衰减因子，节点$v_i,v_j$的Katz指数定义为
\begin{equation}
    S_{ij}=\sum_{l=1}^\infty \beta^l A^l_{ij}.
\end{equation}
矩阵形式为
\begin{equation}
    S=\sum_{l=1}^\infty \beta^l A^l=(I-\beta A)^{-1}-I.
\end{equation}

\subsection{图特征}
\par 图核(graph kernel)给出两个图的相似度: $K(G,G')=\Phi(G)^T \Phi(G')\in \mathbb{R}$.
\par \textbf{图元核}(graphlet kernel)：计数图中图元的个数，这里的图元是无根图，且不一定连通。$K(G,G')=h_G^T h_{G'}$，其中$h_G$是$G$的图元计数向量（用各分量之和归一化）。由于子图同构(subgraph isomorphism)是NP完全问题，此方法计算复杂度过高。

\par \textbf{Weisfeiler-Lehman核}：采用颜色调整法(color refinement)，初始所有节点赋相同颜色$c_0(v)$；每一步根据当前节点和邻接节点颜色Hash到一个新的颜色：
\begin{equation}
    c_{k+1}(v)=\text{HASH}(\{c_k(v),\{c_k(u)\}_{u\in N(v)}\}),
\end{equation}
从而$c_k(v)$汇总了$v$的$k$跳邻域中的信息。Weisfeiler-Lehman核的特征向量是$k$步后各颜色节点的计数向量，\emph{该向量也作为图同构的判别依据}，计算复杂度为$O(\vert E \vert)$.

\section{图嵌入}

\subsection{节点表示}
\par 将图的每个节点映射到一个低维向量，使向量的相似度反映对应节点在图中的相似度。这一过程可取代人工设计的特征，自动提取的表示向量可用于多种下游任务。该向量可直接用于节点分类、聚类；两个节点向量的连接、Hadamard积、加和、距离等可用于链接预测。

\par 从编码-解码的角度看，编码器将一个节点映射为一个$d$维表示向量，解码器从两个向量得到对应节点的相似度（如向量点乘）。简单的编码器可以仅做“查表”，即学习一个$d\times N$矩阵$Z$，每一列为对应节点的表示向量。

\par 节点相似度的一种简单度量方式是其是否邻接，即要求$\mathbf{z}_v^T\mathbf{z}_u=A_{uv}$，$A$为无向图的邻接矩阵；亦即$Z^T Z=A$. 由于$d<<N$，上式难以严格成立，考虑$\min \|A-Z^T Z\|_2$. 可利用矩阵分解求解。

\par 不考虑节点特征，\textbf{随机游走法}将两个节点的相似度定义为它们在一次图上的随机游走中共现的概率。给定随机游走策略$R$，从节点$u$出发进行固定（较短）长度的随机游走，访问节点的多重集记为$N_R(u)$；用softmax函数表示概率，采用最大似然目标函数优化节点表示：
\begin{align}
\mathcal{L}&=-\sum_{u\in V}\sum_{v \in N_R(u)} \log P(v \vert \mathbf{z}_u)\notag \\
&=-\sum_{u\in V}\sum_{v \in N_R(u)}\log \frac{\exp(\mathbf{z}_u^T \mathbf{z}_v)}{\sum_{n\in V}\exp(\mathbf{z}_u^T \mathbf{z}_n)}
\end{align}
利用负采样(negative sampling)，上式每一项可近似为
\begin{equation}
    \log \sigma(\mathbf{z}_u^T \mathbf{z}_v)-\sum_{i=1}^k \log \sigma(\mathbf{z}_u^T \mathbf{z}_{n_i}).
\end{equation}
其中$\sigma(\cdot)$为Sigmoid函数；负样本$\{n_i\}_{i=1}^k$以节点度为概率随机抽样；理论上应取不在该游走上的节点，实际出于效率不加此限制；$k$一般取5-20（过少则估计不准确，过多则产生更大负样本偏差）。采用随机梯度下降优化，每一步随机取节点$u$，对$-\sum_{v \in N_R(u)} \log P(v \vert \mathbf{z}_u)$求梯度，更新各表示向量。

\par \textbf{DeepWalk}采用无偏随机游走。\textbf{node2vec}采用有偏随机游走，以达到网络全局信息(DFS)与局部信息(BFS)的平衡：设$p,q$为参数, 若上一步从$v$走到$w$，则当前步骤以概率$1/p$（非归一化概率，下同）回到$v$；以概率1到达$v$的每个其它相邻节点；以概率$1/q$到达$v$的每个距离为2的节点。$p$低则接近于BFS；$q$低则接近于DFS。

\par 矩阵分解法和随机游走法的局限性：(1)加入新节点时，所有表示向量都要重新计算；(2)不能表达结构相似性（更多关注邻近性）；(3)不能纳入节点和边的特征。图神经网络、深度表示学习可以解决上述问题。

\subsection{图（子图）表示}
\par \textbf{简单加和法}：各节点表示向量求和（求平均）。
\par \textbf{虚拟节点法}：引入一个虚拟节点代替子图，利用节点表示方法得到子图表示。
\par \textbf{匿名随机游走}(anonymous random walk)：随机游走序列不记录具体节点，只记录与序列中其它节点相同或相异（如A-B-C-A-C与C-D-B-C-B均为12313）\footnote{长度为$n$的匿名随机游走的数量是Bell数(OEIS-A000110)，即将$n$个带标记元素划分成若干集合的方法数。}。在图上进行若干次随机游走，用各匿名序列的概率分布作为图的表示；或以预测同一节点出发的相邻匿名序列为目标，同时学习每种匿名序列的表示和图的表示。

\section{消息传递}
\par 社交网络中存在\textbf{相关性}(correlation)或\textbf{依赖性}(dependency)：相近的节点具有相同类别标签。这一现象可从两个角度解释：同质相吸(homophily)即相似的个体更容易产生连接；社会影响(influence)使连接的个体趋向于同质。以此为理论基础，半监督节点分类中节点类别不仅依赖于自身特征，也依赖于邻接节点的类别和特征。消息传递框架下的半监督节点分类任务有如下代表性方法：

\par \textbf{关系分类}(Relational Classification)：利用节点连接关系迭代计算未知节点的类别概率。初始已知节点类别概率为one-hot向量，未知节点各类概率分别记为$1/K$（$K$为类别数）；每轮未知节点类别概率向量更新为邻接节点类别概率向量的加权和（可按边权加权）。收敛后输出概率最大的类别。这一方法没有纳入节点特征，且无收敛性保证。

\par \textbf{迭代分类}(Iterative Classification)：设节点$v$特征为$f_v$，邻域类别标签汇总为向量$z_v$（可以包括各类别计数或占比、出现最多的类别、类别数等,有向图出边入边分别统计），在训练集上训练两个分类器$\phi_1(f_v)$和$\phi_2(f_v,z_v)$；在测试集上先用$\phi_1$赋初始类别，每次迭代先更新所有$z_v$再用$\phi_2$更新测试集的类别标签，直到收敛。只使用了邻接节点类别，且没有收敛性保证。

\par \textbf{修正平滑法}(Correct \& Smooth)：先将有标签节点作为训练集、验证集，训练基于节点特征预测各类别概率的基分类器；之后利用该分类器得到所有节点的类别概率。假设基分类器分类结果的误差具有正自相关，后处理阶段包括修正、平滑两个步骤。
\par 设$D=\operatorname{diag}\{d_1,\dots,d_N\}$为度矩阵，定义归一化扩散矩阵(normalized diffusion matrix)为$\tilde{A}=D^{-1/2}AD^{-1/2}$，其中$A$为邻接矩阵，满足特征值均在[-1,1]，最大特征值为1\footnote{$D^{1/2}\mathbf{1}$是一个对应的特征向量。事实上有$\tilde{A}D^{1/2}\mathbf{1}=D^{-1/2}A\mathbf{1}=D^{-1/2}D\mathbf{1}=D^{1/2}\mathbf{1}$.}；这对$\tilde{A}^m$同样成立。
\par 修正步骤：设$E$为$N\times K$训练误差矩阵，对于标记节点$i$，$E_{ik}$为属于第$k$类的概率真值（0或1）与基分类器预测概率之差；对于非标记节点，对应行为0。按下式更新误差若干次后，将误差的$s$倍加在基分类器预测概率上（$s$为超参数）；
\begin{equation}
    E \leftarrow (1-\alpha)E + \alpha \tilde{A}E.
\end{equation}
\par 平滑步骤：设$Z$为$N\times K$类别概率矩阵，对于标记节点$i$，$E_{ik}$为属于第$k$类的概率真值（0或1）；对于非标记节点，$E_{ik}$为修正步骤的类别概率。按下式更新误差若干次后，对每个节点输出概率最大的类别\footnote{平滑步骤可能使每个节点类别概率和不再为1。}。
\begin{equation}
    Z \leftarrow (1-\alpha)Z + \alpha \tilde{A}Z.
\end{equation}


\section{图神经网络}


\section{图生成模型}


\section{其它主题}
