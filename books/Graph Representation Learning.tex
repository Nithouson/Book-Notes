
\chapter{图表示学习}
\Large\textbf{Graph Representation Learning\footnote{本笔记亦主要参考Jure Leskovec图机器学习讲义(Stanford CS224W，2021)}}
\par \emph{William L. Hamilton} \normalsize

\section{导论}

\par 图是描述和分析有关联(交互)实体的通用语言。图机器学习的核心问题：如何利用关系结构进行更好的预测？

\par 任务分类：
\begin{itemize}
    \item 节点级(Node level)：节点分类
    \item 链接级(Link level)：链接预测（根据已知边预测其它节点对是否连边，如知识图谱补全，推荐系统中预测用户与物品的连接）
    \item 子图级(Subgraph level)：社区探测
    \item 图级(Graph level)：图分类（如分子性质预测），图生成，图演化模拟
\end{itemize}

\par 二部图的投影图(folded/projected graph)：设两个顶点独立集$U,V$，对$U$的投影图节点为$U$，$u_1,u_2\in U$连边当且仅当存在$v\in V$，$u_1,u_2$在二部图中都与$v$相连。

\par 异质图(heterogeneous graph)：区分不同类型节点和边。 $G=(V,E,R,T)$，边定义为$(v_i,r_{ij},v_j)\in E$, $r_{ij}\in R$是边类型，$T(v_i)$是节点类型。在知识图谱中常用。

\par 稀疏图(sparse graph)：平均度远小于$N-1$。大多数现实世界的网络都是稀疏图。

\section{传统方法}

\par 传统图机器学习方法需要人为定义节点、节点对、图的特征（表示为向量）用于训练。

\subsection{节点特征}

度、中心性描述节点的重要性。度、聚集系数、图元度向量描述节点所在局部的拓扑属性。

\par \textbf{度}(degree).

\par \textbf{中心性}(centrality): 衡量节点在图中的重要性。
\par 特征向量中心性(eigenvector centrality)满足
\begin{equation}
    c_v=\frac{1}{\lambda}\sum_{u\in N(v)} c_u.
\end{equation}
其中$A$为无向图的邻接矩阵。上式化为$\lambda \mathbf{c}=A\mathbf{c}$，即$\mathbf{c}$为$A$的特征向量。一般取最大特征值对应特征向量\footnote{无向连通图的邻接矩阵不可约。Perron-Frobenius定理指出，不可约非负矩阵最大特征值为正且代数重数为1，从而该特征向量在乘一个常数意义下唯一。}。
\par 介中心性(betweenness centrality)：对节点$v$，定义为一对节点所有最短路径中包含$v$的路径所占比例，对所有不含$v$的节点对求和。
\par 接近中心性(closeness centrality)：对给定节点，定义为该节点到其余节点最短路径长度之和的倒数。

\par \textbf{聚集系数}(clustering coefficient)：对节点$v$，考虑其邻接节点（设有$k_v$个）的导出子图，其边数与$\binom{k_v}{2}$之比。反映邻域连通程度。
\par \textbf{图元度向量}(graphlet degree vector)：图元即较小的连通有根图。令根节点与给定节点对应，对每个图元统计与之同构的导出子图的数量\footnote{如考虑节点数2-5的图元，得到73维向量。}。事实上度统计$K_2$的数量，聚集系数统计$K_3$的数量。


\subsection{节点对特征}

\par \textbf{距离}：网络最短距离。
\par \textbf{局部邻域重叠}：公共邻居数；
\par Jaccard指数：
\begin{equation}
    \frac{\vert N(v_1)\cap N(v_2)\vert}{\vert N(v_1)\cup N(v_2)\vert}
\end{equation}
Adamic-Adar指数：
\begin{equation}
    \sum_{u\in N(v_1)\cap N(v_2)} \frac{1}{\log k_u}.
\end{equation}
局限性是如果没有公共邻居，局部邻域重叠总是0.

\par \textbf{Katz指数}：对简单无向图邻接矩阵乘方，可得到一对节点间给定长度的路径数。设$\beta \in (0,1)$为衰减因子，节点$v_i,v_j$的Katz指数定义为
\begin{equation}
    S_{ij}=\sum_{l=1}^\infty \beta^l A^l_{ij}.
\end{equation}
矩阵形式为
\begin{equation}
    S=\sum_{l=1}^\infty \beta^l A^l=(I-\beta A)^{-1}-I.
\end{equation}

\subsection{图特征}
\par 图核(graph kernel)给出两个图的相似度: $K(G,G')=\Phi(G)^T \Phi(G')\in \mathbb{R}$.
\par \textbf{图元核}(graphlet kernel)：计数图中图元的个数，这里的图元是无根图，且不一定连通。$K(G,G')=h_G^T h_{G'}$，其中$h_G$是$G$的图元计数向量（用各分量之和归一化）。由于子图同构(subgraph isomorphism)是NP完全问题，此方法计算复杂度过高。

\par \textbf{Weisfeiler-Lehman核}：采用颜色调整法(color refinement)，初始所有节点赋相同颜色$c_0(v)$；每一步根据当前节点和邻接节点颜色Hash到一个新的颜色：
\begin{equation}
    c_{k+1}(v)=\text{HASH}(\{c_k(v),\{c_k(u)\}_{u\in N(v)}\}),
\end{equation}
从而$c_k(v)$汇总了$v$的$k$跳邻域中的信息。Weisfeiler-Lehman核的特征向量是$k$步后各颜色节点的计数向量，\emph{该向量也作为图同构的判别依据}，计算复杂度为$O(\vert E \vert)$.

\section{图嵌入}

\subsection{节点表示}
\par 将图的每个节点映射到一个低维向量，使向量的相似度反映对应节点在图中的相似度。这一过程可取代人工设计的特征，自动提取的表示向量可用于多种下游任务。

\par 从编码-解码的角度看，编码器将一个节点映射为一个$d$维表示向量，解码器从两个向量得到对应节点的相似度（如向量点乘）。简单的编码器可以仅做“查表”，即学习一个$d\times N$矩阵，每一列为对应节点的表示向量。

\par 不考虑节点特征，\textbf{随机游走法}将两个节点的相似度定义为它们在一次图上的随机游走中共现的概率。给定随机游走策略$R$，从节点$u$出发进行固定（较短）长度的随机游走，访问节点的多重集记为$N_R(u)$；用softmax函数表示概率，采用最大似然目标函数优化节点表示：
\begin{align}
\mathcal{L}&=-\sum_{u\in V}\sum_{v \in N_R(u)} \log P(v \vert \mathbf{z}_u)\notag \\
&=-\sum_{u\in V}\sum_{v \in N_R(u)}\log \frac{\exp(\mathbf{z}_u^T \mathbf{z}_v)}{\sum_{n\in V}\exp(\mathbf{z}_u^T \mathbf{z}_n)}
\end{align}
利用负采样(negative sampling)，上式每一项可近似为
\begin{equation}
    \log \sigma(\mathbf{z}_u^T \mathbf{z}_v)-\sum_{i=1}^k \log \sigma(\mathbf{z}_u^T \mathbf{z}_{n_i}).
\end{equation}
其中$\sigma(\cdot)$为Sigmoid函数；负样本$\{n_i\}_{i=1}^k$以节点度为概率随机抽样；理论上应取不在该游走上的节点，实际出于效率不加此限制；$k$一般取5-20（过少则估计不准确，过多则产生更大负样本偏差）。采用随机梯度下降优化，每一步随机取节点$u$，对$-\sum_{v \in N_R(u)} \log P(v \vert \mathbf{z}_u)$求梯度，更新各表示向量。

\par \textbf{DeepWalk}采用无偏随机游走。\textbf{node2vec}采用有偏随机游走，以达到网络全局信息(DFS)与局部信息(BFS)的平衡：设$p,q$为参数, 若上一步从$v$走到$w$，则当前步骤以概率$1/p$（非归一化概率，下同）回到$v$；以概率1到达$v$的每个其它相邻节点；以概率$1/q$到达$v$的每个距离为2的节点。$p$低则接近于BFS；$q$低则接近于DFS。

\subsection{图（子图）表示}
\par \textbf{简单加和法}：各节点表示向量求和（求平均）。
\par \textbf{虚拟节点法}：引入一个虚拟节点代替子图，利用节点表示方法得到子图表示。
\par \textbf{匿名随机游走}(anonymous random walk)：随机游走序列不记录具体节点，只记录与序列中其它节点相同或相异（如A-B-C-A-C与C-D-B-C-B均为12313）。在图上进行若干次随机游走，用各匿名序列的概率分布作为图的表示。

\section{图神经网络}

\section{图生成模型}

\section{其它主题}
